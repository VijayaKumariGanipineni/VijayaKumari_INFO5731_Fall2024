{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VijayaKumariGanipineni/VijayaKumari_INFO5731_Fall2024/blob/main/Ganipineni_VijayaKumari_InClassExercise3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "70110c99-7bc4-41b7-a1fd-8005ed3ac336"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPlease write you answer here:\\n\\n1.Choose a text classification project that includes sentiment analysis for the purpose of determining whether textual information indicates a positive or negative viewpoint of the data we have taken.\\n  Sentiment analysis is a typical text type venture that analyzes textual content details to ascertain the author's sentiment.\\n  It is frequently used to get purchaser remarks, observe products, and reveal social media.\\n2.To extract terms depend upon competencies, sum the wide variety of terms in each sentence in the record.\\n  Explanation: Word be matter can imply text period and records intensity, which also can align with evaluations. Longer discussions can also \\n  bring more history, influencing intellectual excellence.\\n3. To figure out the significance of an expression in a collection of files, use the frequency-inverse report frequency (TF-IDF) computation.\\n  Explanation: TF-IDF can efficiently categorize data by reducing the weight of frequently used terms while boosting the weight of essential phrases.\\n4.Use element-of-speech (POS) tagging to identify grammar elements of phrases such as nouns, verbs, and adjectives. \\n  Explanation: POS identification can help discern between different meanings of a term, potentially influencing mentality. Adjectives are effective\\n   markers of mood.\\n5.Analyze the sentiment rating of each text using a sentiment lexicon to identify the overall sentiment polarity.\\n  Explanation:\\n  A sentiment lexicon carries lists of phrases with associated sentiment ratings. By aggregating these rankings, you could get an\\n   normal sentiment estimation for the text.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "1.Choose a text classification project that includes sentiment analysis for the purpose of determining whether textual information indicates a positive or negative viewpoint of the data we have taken.\n",
        "  Sentiment analysis is a typical text type venture that analyzes textual content details to ascertain the author's sentiment.\n",
        "  It is frequently used to get purchaser remarks, observe products, and reveal social media.\n",
        "2.To extract terms depend upon competencies, sum the wide variety of terms in each sentence in the record.\n",
        "  Explanation: Word be matter can imply text period and records intensity, which also can align with evaluations. Longer discussions can also\n",
        "  bring more history, influencing intellectual excellence.\n",
        "3. To figure out the significance of an expression in a collection of files, use the frequency-inverse report frequency (TF-IDF) computation.\n",
        "  Explanation: TF-IDF can efficiently categorize data by reducing the weight of frequently used terms while boosting the weight of essential phrases.\n",
        "4.Use element-of-speech (POS) tagging to identify grammar elements of phrases such as nouns, verbs, and adjectives.\n",
        "  Explanation: POS identification can help discern between different meanings of a term, potentially influencing mentality. Adjectives are effective\n",
        "   markers of mood.\n",
        "5.Analyze the sentiment rating of each text using a sentiment lexicon to identify the overall sentiment polarity.\n",
        "  Explanation:\n",
        "  A sentiment lexicon carries lists of phrases with associated sentiment ratings. By aggregating these rankings, you could get an\n",
        "   normal sentiment estimation for the text.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eb34c6d-34b0-4950-fba6-94be30c51f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Text  Sentiment Score  \\\n",
            "0  I love this product! It works wonderfully and ...           0.8553   \n",
            "1  This is the worst experience I've ever had. I'...          -0.8173   \n",
            "2  The service was okay, but the food was fantastic!           0.7678   \n",
            "3  I would not recommend this to anyone. It was a...          -0.6007   \n",
            "4  Absolutely amazing! I will definitely buy this...           0.8100   \n",
            "\n",
            "   Term Count                                           POS Tags  \n",
            "0          14  [(I, PRP), (love, VBP), (this, DT), (product, ...  \n",
            "1          15  [(This, DT), (is, VBZ), (the, DT), (worst, JJS...  \n",
            "2          11  [(The, DT), (service, NN), (was, VBD), (okay, ...  \n",
            "3          16  [(I, PRP), (would, MD), (not, RB), (recommend,...  \n",
            "4          10  [(Absolutely, RB), (amazing, VBG), (!, .), (I,...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk import download\n",
        "\n",
        "# Sample text data\n",
        "textdata = [\n",
        "    \"I love this product! It works wonderfully and has changed my life.\",\n",
        "    \"This is the worst experience I've ever had. I'm very disappointed.\",\n",
        "    \"The service was okay, but the food was fantastic!\",\n",
        "    \"I would not recommend this to anyone. It was a complete waste of time.\",\n",
        "    \"Absolutely amazing! I will definitely buy this again.\"\n",
        "]\n",
        "\n",
        "# 1. Sentiment Analysis\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = [sia.polarity_scores(text) for text in textdata]\n",
        "\n",
        "# 2. Count the number of terms in each sentence\n",
        "term_counts = [len(word_tokenize(text)) for text in textdata]\n",
        "\n",
        "# 3. TF-IDF computation\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(textdata)\n",
        "\n",
        "# 4. Part-of-Speech (POS) tagging\n",
        "pos_tags = [pos_tag(word_tokenize(text)) for text in textdata]\n",
        "\n",
        "# 5. Aggregate sentiment ratings\n",
        "sentiment_ratings = [score['compound'] for score in sentiment_scores]\n",
        "\n",
        "# Create a DataFrame to display results\n",
        "extract_features = pd.DataFrame({\n",
        "    'Text': textdata,\n",
        "    'Sentiment Score': sentiment_ratings,\n",
        "    'Term Count': term_counts,\n",
        "    'POS Tags': pos_tags\n",
        "})\n",
        "\n",
        "print(extract_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "335d7778-820a-4730-b751-d4b15e86b3c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Feature     Score\n",
            "40         would  1.500000\n",
            "10  disappointed  1.500000\n",
            "26     recommend  1.500000\n",
            "17            is  1.500000\n",
            "30          time  1.500000\n",
            "15           had  1.500000\n",
            "31            to  1.500000\n",
            "32            ve  1.500000\n",
            "12    experience  1.500000\n",
            "23            of  1.500000\n",
            "11          ever  1.500000\n",
            "33          very  1.500000\n",
            "8       complete  1.500000\n",
            "35         waste  1.500000\n",
            "4         anyone  1.500000\n",
            "39         worst  1.500000\n",
            "22           not  1.500000\n",
            "36          will  0.666667\n",
            "37   wonderfully  0.666667\n",
            "38         works  0.666667\n",
            "27       service  0.666667\n",
            "25       product  0.666667\n",
            "24          okay  0.666667\n",
            "0     absolutely  0.666667\n",
            "21            my  0.666667\n",
            "1          again  0.666667\n",
            "2        amazing  0.666667\n",
            "3            and  0.666667\n",
            "5            but  0.666667\n",
            "6            buy  0.666667\n",
            "7        changed  0.666667\n",
            "9     definitely  0.666667\n",
            "13     fantastic  0.666667\n",
            "14          food  0.666667\n",
            "16           has  0.666667\n",
            "19          life  0.666667\n",
            "20          love  0.666667\n",
            "29          this  0.166667\n",
            "18            it  0.083333\n",
            "34           was  0.055556\n",
            "28           the  0.055556\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "#Data I have taken\n",
        "textdata = [\n",
        "    \"I love this product! It works wonderfully and has changed my life.\",\n",
        "    \"This is the worst experience I've ever had. I'm very disappointed.\",\n",
        "    \"The service was okay, but the food was fantastic!\",\n",
        "    \"I would not recommend this to anyone. It was a complete waste of time.\",\n",
        "    \"Absolutely amazing! I will definitely buy this again.\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(textdata)\n",
        "y = [1, 0, 1, 0, 1]  # Example labels: 1 for positive, 0 for negative\n",
        "\n",
        "selector = SelectKBest(score_func=chi2, k='all')\n",
        "selector.fit(X, y)\n",
        "\n",
        "scores = selector.scores_\n",
        "features = vectorizer.get_feature_names_out()\n",
        "#seperating the feature and the score\n",
        "feature_importance = pd.DataFrame({'Feature': features, 'Score': scores})\n",
        "feature_importance = feature_importance.sort_values(by='Score', ascending=False)\n",
        "#printing features and score\n",
        "print(feature_importance)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaffaeb1-a1e7-4204-8326-6da623abf8ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar texts are arranged: \n",
            "Similarity Score: 0.5601328611373901 <|> Text: I would not recommend this to anyone. It was a complete waste of time.\n",
            "Similarity Score: 0.5489635467529297 <|> Text: I love this product! It works wonderfully and has changed my life.\n",
            "Similarity Score: 0.488338828086853 <|> Text: The service was okay, but the food was fantastic!\n",
            "Similarity Score: 0.47442173957824707 <|> Text: This is the worst experience I've ever had. I'm very disappointed.\n",
            "Similarity Score: 0.46073293685913086 <|> Text: Absolutely amazing! I will definitely buy this again.\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "# Installing the transformers library since am  running in a new environment)\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# My text data\n",
        "textdata = [ \"I love this product! It works wonderfully and has changed my life.\",\n",
        "    \"This is the worst experience I've ever had. I'm very disappointed.\",\n",
        "    \"The service was okay, but the food was fantastic!\",\n",
        "    \"I would not recommend this to anyone. It was a complete waste of time.\",\n",
        "    \"Absolutely amazing! I will definitely buy this again.\"]\n",
        "\n",
        "#a query for matching\n",
        "query = \"Datascience is the highest required course for the Generative AI.\"\n",
        "\n",
        "# Loading the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# get_bert_embedding function to get BERT embeddings for a given text\n",
        "def get_bert_embedding(text):\n",
        "    inputtexttaken = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "    outputresult = model(**inputtexttaken)\n",
        "    return outputresult.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Getting the BERT embedding for the query\n",
        "query_embedding = get_bert_embedding(query)\n",
        "\n",
        "# calculating cosine similarities. This is between the query and each text\n",
        "similartexts= []\n",
        "for text in textdata:\n",
        "    text_embedding = get_bert_embedding(text)\n",
        "    similarity = cosine_similarity(query_embedding, text_embedding)[0][0]\n",
        "    similartexts.append((text, similarity))\n",
        "\n",
        "# Placing the texts in descending order\n",
        "similartexts.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Printing the ranked texts based on similarity to the query\n",
        "print(\"Similar texts are arranged: \")\n",
        "for text, score in similartexts:\n",
        "    print(f\"Similarity Score: {score} <|> Text: {text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Learning Experience: This mission provides a very difficult evaluation of text content function extraction techniques\n",
        "in relation to various NLP jobs and the demanded approach. Utilisation of a bag of words, TF-IDF, sentiment lexicons as well as advanced techniques\n",
        "like embedding words and parsing dependencies were very fruitful. The toes-on character of acquiring those skills in Python made me keep in mind more\n",
        " or less how they will be applicable.\n",
        "\n",
        "Challenges Encountered: It turned out to be a hard task to make more and more sophisticated features such as dependencies and word vectors,\n",
        "especially to find libraries that are suitable and understand what the effects are.\n",
        "\n",
        "Relevance to NLP: This exercise is absorbing to the extent that it finds its application in text analysis tasks where it serves as the basis\n",
        "for multitude other. The ability to transform the raw text and view it into a representation suitable for machine learning models emphasis the\n",
        "importance of feature extraction. The move from fundamental word-based features to more complex linguistic and semantic features signals that\n",
        "understanding of NLP techniques is far improving sophistication and the potential of this area of study.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "2bf3320e-94d9-4c9f-ee6d-3a6cc2614716"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\nLearning Experience: This mission provides a very difficult evaluation of text content function extraction techniques \\nin relation to various NLP jobs and the demanded approach. Utilisation of a bag of words, TF-IDF, sentiment lexicons as well as advanced techniques \\nlike embedding words and parsing dependencies were very fruitful. The toes-on character of acquiring those skills in Python made me keep in mind more\\n or less how they will be applicable.\\n\\nChallenges Encountered: It turned out to be a hard task to make more and more sophisticated features such as dependencies and word vectors, \\nespecially to find libraries that are suitable and understand what the effects are.\\n\\nRelevance to NLP: This exercise is absorbing to the extent that it finds its application in text analysis tasks where it serves as the basis \\nfor multitude other. The ability to transform the raw text and view it into a representation suitable for machine learning models emphasis the \\nimportance of feature extraction. The move from fundamental word-based features to more complex linguistic and semantic features signals that \\nunderstanding of NLP techniques is far improving sophistication and the potential of this area of study.\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}