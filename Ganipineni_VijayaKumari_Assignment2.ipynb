{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VijayaKumariGanipineni/VijayaKumari_INFO5731_Fall2024/blob/main/Ganipineni_VijayaKumari_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing required libraries\n",
        "!pip install requests beautifulsoup4 pandas\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_imdb_reviews(url, max_reviews=1000):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
        "    }\n",
        "    reviews = []\n",
        "    page = 0\n",
        "\n",
        "    while len(reviews) < max_reviews:\n",
        "        # Constructing the URL for the review pages (IMDB uses offsets for pages)\n",
        "        review_url = f\"{url}?ref_=undefined&paginationKey={page}\"\n",
        "        print(f\"Visiting {review_url}\")\n",
        "\n",
        "        response = requests.get(review_url, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            print(\"Failed to retrieve page. Exiting...\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        review_elements = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "        for review in review_elements:\n",
        "            try:\n",
        "                review_text = review.text.strip()\n",
        "                reviews.append({\n",
        "                    'Review': review_text\n",
        "                })\n",
        "\n",
        "                if len(reviews) >= max_reviews:\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f\"Error while parsing review: {e}\")\n",
        "\n",
        "        page += 1\n",
        "\n",
        "    return reviews\n",
        "\n",
        "# URL of the IMDB movie reviews page\n",
        "imdb_url = \"https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2\"  #  URL for movie\n",
        "imdb_reviews_data = get_imdb_reviews(imdb_url, max_reviews=1000)\n",
        "\n",
        "# Saving to CSV\n",
        "df_imdb_reviews = pd.DataFrame(imdb_reviews_data)\n",
        "df_imdb_reviews.to_csv('movie_reviews.csv', index=False)\n",
        "print(f\"Saved {len(df_imdb_reviews)} reviews to 'movie_reviews.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59Ox3jp1_b53",
        "outputId": "e17e0f0f-a8b9-47ba-9393-9106910d5045"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=0\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=1\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=2\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=3\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=4\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=5\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=6\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=7\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=8\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=9\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=10\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=11\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=12\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=13\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=14\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=15\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=16\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=17\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=18\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=19\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=20\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=21\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=22\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=23\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=24\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=25\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=26\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=27\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=28\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=29\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=30\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=31\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=32\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=33\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=34\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=35\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=36\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=37\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=38\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=39\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=40\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=41\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=42\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=43\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=44\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=45\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=46\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=47\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=48\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=49\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=50\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=51\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=52\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=53\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=54\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=55\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=56\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=57\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=58\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=59\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=60\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=61\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=62\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=63\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=64\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=65\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=66\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=67\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=68\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=69\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=70\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=71\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=72\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=73\n",
            "Visiting https://www.imdb.com/title/tt17526714/reviews/?ref_=tt_ov_ql_2?ref_=undefined&paginationKey=74\n",
            "Saved 1000 reviews to 'movie_reviews.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238492c7-ab77-45bf-8a4d-e24929676ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  \\\n",
            "0  Let me start out by saying: I love body horror...   \n",
            "1  12 minutes of standing ovation during Cannes F...   \n",
            "2  I was extremely hyped for that movie even thou...   \n",
            "3  And I've seen thousands. I estimate about 5k m...   \n",
            "4  Every scene of this film wowed me at TIFF. The...   \n",
            "\n",
            "                                      Cleaned Review  \n",
            "0  let start say love bodi horror dont your squea...  \n",
            "1  minut stand ovat cann film festiv premier demi...  \n",
            "2  extrem hype movi even though im big fan fargea...  \n",
            "3  ive seen thousand estim k movi year earth cant...  \n",
            "4  everi scene film wow tiff cast atmospher visua...  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Installing necessary libraries\n",
        "!pip install nltk pandas\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Downloading nltk resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Loading the IMDB reviews CSV file\n",
        "df = pd.read_csv('movie_reviews.csv')\n",
        "\n",
        "# Function to clean the text data\n",
        "def clean_text(text):\n",
        "    # (1) Removing noise (special characters and punctuations)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # (2) Removing numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # (3) Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # (4) Lowercasing all texts\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # (5) Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # (6) Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
        "\n",
        "    # Joining the words back into a single string\n",
        "    clean_text = ' '.join(lemmatized_words)\n",
        "    return clean_text\n",
        "\n",
        "# Applying the cleaning function to the review text\n",
        "df['Cleaned Review'] = df['Review'].apply(clean_text)\n",
        "\n",
        "# Saving the cleaned data into a new CSV file\n",
        "df.to_csv('cleaned_movie_reviews.csv', index=False)\n",
        "\n",
        "# Displaying the first few rows of the cleaned data\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the cleaned IMDB reviews CSV file\n",
        "df = pd.read_csv('cleaned_movie_reviews.csv')\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function for Parts of Speech (POS) Tagging\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    return pos_tags\n",
        "\n",
        "# Function to count POS tags (Nouns, Verbs, Adjectives, Adverbs)\n",
        "def count_pos(pos_tags):\n",
        "    counts = Counter(tag for word, tag in pos_tags)\n",
        "    return counts\n",
        "\n",
        "# Function for Dependency Parsing and Constituency Parsing\n",
        "def parse_sentence(text):\n",
        "    doc = nlp(text)\n",
        "    return doc\n",
        "\n",
        "# Function for Named Entity Recognition (NER)\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Perform analysis on the first review as an example\n",
        "review_text = df['Cleaned Review'][0]\n",
        "\n",
        "# 1. Parts of Speech Tagging\n",
        "pos_tags = pos_tagging(review_text)\n",
        "print(f\"POS Tags: {pos_tags}\")\n",
        "\n",
        "# Count total number of Nouns, Verbs, Adjectives, Adverbs\n",
        "pos_counts = count_pos(pos_tags)\n",
        "print(f\"POS Counts: {pos_counts}\")\n",
        "\n",
        "# 2. Dependency Parsing and Constituency Parsing\n",
        "parsed_doc = parse_sentence(review_text)\n",
        "print(f\"Dependency Parsing: {[f'{token.text}: {token.dep_}' for token in parsed_doc]}\")\n",
        "\n",
        "# Display the Constituency Parse Tree (spaCy doesn't provide direct constituency parsing)\n",
        "# But here's how we interpret dependencies:\n",
        "for token in parsed_doc:\n",
        "    print(f'{token.text} -> {token.head.text} ({token.dep_})')\n",
        "\n",
        "# 3. Named Entity Recognition (NER)\n",
        "entities = named_entity_recognition(review_text)\n",
        "print(f\"Named Entities: {entities}\")\n",
        "\n",
        "# Count the named entities\n",
        "entity_count = Counter([label for text, label in entities])\n",
        "print(f\"Entity Counts: {entity_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXOPv-W3CBaN",
        "outputId": "964cde83-8c5f-475f-efa3-a4a1fb735837"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('let', 'VB'), ('start', 'VB'), ('say', 'VB'), ('love', 'VB'), ('bodi', 'JJ'), ('horror', 'NN'), ('dont', 'VB'), ('your', 'PRP$'), ('squeamish', 'JJ'), ('might', 'MD'), ('want', 'VB'), ('pas', 'NN'), ('film', 'NN'), ('said', 'VBD'), ('thought', 'VBN'), ('balanc', 'NN'), ('disturb', 'NN'), ('impact', 'NN'), ('intrigu', 'JJ'), ('disgust', 'NN'), ('absolut', 'NN'), ('right', 'RB'), ('moneyin', 'JJ'), ('world', 'NN'), ('full', 'JJ'), ('filler', 'NN'), ('botox', 'NN'), ('face', 'NN'), ('lift', 'NN'), ('implant', 'JJ'), ('skin', 'NN'), ('care', 'NN'), ('routin', 'NN'), ('ob', 'IN'), ('youth', 'NN'), ('peak', 'JJ'), ('medium', 'NN'), ('substanc', 'NN'), ('call', 'NN'), ('question', 'NN'), ('whether', 'IN'), ('new', 'JJ'), ('better', 'JJR'), ('realli', 'NN'), ('one', 'CD'), ('perhap', 'NN'), ('clever', 'NN'), ('gross', 'JJ'), ('point', 'NN'), ('fun', 'NN'), ('watch', 'NN'), ('moor', 'NN'), ('qualley', 'NN'), ('excel', 'NN'), ('duo', 'NN'), ('quaid', 'VBD'), ('play', 'NN'), ('hollywood', 'NN'), ('sleezebal', 'JJ'), ('believ', 'NN'), ('could', 'MD'), ('done', 'VBN')]\n",
            "POS Counts: Counter({'NN': 34, 'JJ': 10, 'VB': 6, 'MD': 2, 'VBD': 2, 'VBN': 2, 'IN': 2, 'PRP$': 1, 'RB': 1, 'JJR': 1, 'CD': 1})\n",
            "Dependency Parsing: ['let: ROOT', 'start: nsubj', 'say: ccomp', 'love: compound', 'bodi: compound', 'horror: nsubj', 'do: aux', 'nt: neg', 'your: poss', 'squeamish: nsubj', 'might: aux', 'want: ccomp', 'pas: compound', 'film: nsubj', 'said: ccomp', 'thought: compound', 'balanc: compound', 'disturb: amod', 'impact: nsubj', 'intrigu: ccomp', 'disgust: amod', 'absolut: dobj', 'right: amod', 'moneyin: compound', 'world: dobj', 'full: amod', 'filler: compound', 'botox: appos', 'face: conj', 'lift: conj', 'implant: amod', 'skin: compound', 'care: compound', 'routin: dobj', 'ob: prep', 'youth: compound', 'peak: nmod', 'medium: amod', 'substanc: pobj', 'call: compound', 'question: nsubj', 'whether: mark', 'new: amod', 'better: amod', 'realli: nmod', 'one: nummod', 'perhap: npadvmod', 'clever: amod', 'gross: amod', 'point: compound', 'fun: compound', 'watch: compound', 'moor: nsubj', 'qualley: compound', 'excel: acl', 'duo: compound', 'quaid: nsubj', 'play: acl', 'hollywood: compound', 'sleezebal: amod', 'believ: dobj', 'could: aux', 'done: ccomp']\n",
            "let -> let (ROOT)\n",
            "start -> say (nsubj)\n",
            "say -> let (ccomp)\n",
            "love -> bodi (compound)\n",
            "bodi -> horror (compound)\n",
            "horror -> want (nsubj)\n",
            "do -> want (aux)\n",
            "nt -> want (neg)\n",
            "your -> squeamish (poss)\n",
            "squeamish -> want (nsubj)\n",
            "might -> want (aux)\n",
            "want -> say (ccomp)\n",
            "pas -> film (compound)\n",
            "film -> said (nsubj)\n",
            "said -> want (ccomp)\n",
            "thought -> impact (compound)\n",
            "balanc -> impact (compound)\n",
            "disturb -> impact (amod)\n",
            "impact -> intrigu (nsubj)\n",
            "intrigu -> said (ccomp)\n",
            "disgust -> absolut (amod)\n",
            "absolut -> intrigu (dobj)\n",
            "right -> world (amod)\n",
            "moneyin -> world (compound)\n",
            "world -> intrigu (dobj)\n",
            "full -> filler (amod)\n",
            "filler -> botox (compound)\n",
            "botox -> world (appos)\n",
            "face -> intrigu (conj)\n",
            "lift -> intrigu (conj)\n",
            "implant -> routin (amod)\n",
            "skin -> care (compound)\n",
            "care -> routin (compound)\n",
            "routin -> lift (dobj)\n",
            "ob -> routin (prep)\n",
            "youth -> peak (compound)\n",
            "peak -> substanc (nmod)\n",
            "medium -> substanc (amod)\n",
            "substanc -> ob (pobj)\n",
            "call -> question (compound)\n",
            "question -> done (nsubj)\n",
            "whether -> excel (mark)\n",
            "new -> moor (amod)\n",
            "better -> realli (amod)\n",
            "realli -> fun (nmod)\n",
            "one -> realli (nummod)\n",
            "perhap -> clever (npadvmod)\n",
            "clever -> fun (amod)\n",
            "gross -> point (amod)\n",
            "point -> fun (compound)\n",
            "fun -> watch (compound)\n",
            "watch -> moor (compound)\n",
            "moor -> excel (nsubj)\n",
            "qualley -> excel (compound)\n",
            "excel -> question (acl)\n",
            "duo -> quaid (compound)\n",
            "quaid -> play (nsubj)\n",
            "play -> question (acl)\n",
            "hollywood -> believ (compound)\n",
            "sleezebal -> believ (amod)\n",
            "believ -> play (dobj)\n",
            "could -> done (aux)\n",
            "done -> let (ccomp)\n",
            "Named Entities: [('one', 'CARDINAL'), ('duo quaid', 'PERSON')]\n",
            "Entity Counts: Counter({'CARDINAL': 1, 'PERSON': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cleaned_movie_reviews.csv')"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "da0a44fc-dc6a-474a-f174-e87aff7a3b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bebcc7ac-13d6-4894-8526-39001d2ae79b\", \"cleaned_movie_reviews.csv\", 3465702)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "'''\n",
        "The assignment was relevant although I faced a lot of challenges accessing some information from the required\n",
        "websites. For example, the page for the narrators was access forbiden and the articles abstracts too page was blocking\n",
        "and therefore settled on the movies. I enjoyed data scaping and the cleaning of data alot. In future, I would suggest\n",
        "to be given a wider range of getting data.\n",
        "'''"
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "4399b0f6-9701-4660-f22f-9333a209c04b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe assignment was relevant although I faced a lot of challenges accessing some information from the required\\nwebsites. For example, the page for the narrators was access forbiden and the articles abstracts too page was blocking\\nand therefore settled on the movies. I enjoyed data scaping and the cleaning of data alot. In future, I would suggest\\nto be given a wider range of getting data.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}